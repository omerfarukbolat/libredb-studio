# ============================================
# LibreDB Studio - Environment Configuration
# ============================================
#
# LOCAL DEVELOPMENT:
#   Copy this file to .env.local and fill in your values
#   cp .env.example .env.local
#
# DOCKER / RENDER DEPLOYMENT:
#   Set these variables in your deployment environment
#
# ============================================

# ============================================
# AUTHENTICATION (Required)
# ============================================
# Password for admin access (full access + maintenance tools)
ADMIN_PASSWORD=your_secure_admin_password

# Password for user access (query execution only)
USER_PASSWORD=your_secure_user_password

# JWT Secret for session management (min 32 characters)
# Generate with: openssl rand -base64 32
JWT_SECRET=your_32_character_random_string_here

# ===========================================
# LLM Configuration (Strategy Pattern)
# ===========================================
# Provider options: gemini, openai, ollama, custom
# The system uses Strategy Pattern to automatically select
# the appropriate provider based on this configuration.
LLM_PROVIDER=gemini

# API Key for the selected provider
# Required for: gemini, openai
# Optional for: ollama, custom (depends on endpoint)
#
# Get API keys from:
# - Gemini: https://aistudio.google.com/
# - OpenAI: https://platform.openai.com/
LLM_API_KEY=your_api_key_here

# Model name (optional - auto-defaults based on provider)
# Default models:
# - Gemini: gemini-2.5-flash
# - OpenAI: gpt-4o
# - Ollama: llama3.2
# - Custom: gpt-3.5-turbo
#
# Popular options:
# Gemini: gemini-2.5-flash, gemini-2.0-flash, gemini-1.5-flash, gemini-1.5-pro
# OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, gpt-4o-mini
# Ollama: llama3.2, mistral, codellama, deepseek-coder
LLM_MODEL=gemini-2.5-flash

# API URL (optional - only needed for ollama or custom providers)
# Default URLs:
# - Ollama: http://localhost:11434/v1
# - OpenAI: https://api.openai.com/v1
#
# Custom provider examples:
# - LiteLLM: http://localhost:4000/v1
# - LMStudio: http://localhost:1234/v1
# - vLLM: http://localhost:8000/v1
# - LocalAI: http://localhost:8080/v1
LLM_API_URL=http://localhost:11434/v1

# ===========================================
# Provider Configuration Examples
# ===========================================

# --- Gemini (Default) ---
# LLM_PROVIDER=gemini
# LLM_API_KEY=AIzaSy...
# LLM_MODEL=gemini-2.5-flash

# --- OpenAI ---
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-...
# LLM_MODEL=gpt-4o

# --- Ollama (Local) ---
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
# LLM_API_URL=http://localhost:11434/v1

# --- LiteLLM Proxy ---
# LLM_PROVIDER=custom
# LLM_API_KEY=your_litellm_key  # optional
# LLM_MODEL=gpt-4o
# LLM_API_URL=http://localhost:4000/v1

# --- LMStudio (Local) ---
# LLM_PROVIDER=custom
# LLM_MODEL=local-model
# LLM_API_URL=http://localhost:1234/v1

# ===========================================
# Demo Database Configuration (Optional)
# ===========================================
# Enable this to provide a pre-configured demo database
# that users can immediately explore when they open the app.
# Perfect for product demos, onboarding, and trial experiences.
#
# When enabled, a "Demo Database" connection will automatically
# appear in the connections list (cannot be deleted/edited).

# Enable/disable demo database feature
DEMO_DB_ENABLED=false

# Display name shown in the connections list
DEMO_DB_NAME=Employee PostgreSQL (Demo)

# PostgreSQL connection details
# IMPORTANT: Use 'employees' database, NOT 'neondb'
DEMO_DB_HOST=your-neon-host.neon.tech
DEMO_DB_PORT=5432
DEMO_DB_DATABASE=employees
DEMO_DB_USER=employees_readonly
DEMO_DB_PASSWORD=your_demo_password

# --- Example with Neon Cloud ---
# DEMO_DB_ENABLED=true
# DEMO_DB_NAME=Employee PostgreSQL (Demo)
# DEMO_DB_HOST=ep-xxx-xxx-pooler.region.aws.neon.tech
# DEMO_DB_PORT=5432
# DEMO_DB_DATABASE=employees
# DEMO_DB_USER=employees_readonly
# DEMO_DB_PASSWORD=readonly_secure_pass
